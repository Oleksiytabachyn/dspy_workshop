{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from utils.helper import get_api_key, validate_prediction, load_data, ExperimentStats\n",
    "import dspy\n",
    "import mlflow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If the cell below takes to long, please check terminal with `mlflow` server running.\n",
    "\n",
    "Spin up MLflow server by command:\n",
    "\n",
    "```bash\n",
    "mlflow server --backend-store-uri sqlite:///data/mlflow.db --port 5005\n",
    "```"
   ],
   "id": "201d1f82a1df578"
  },
  {
   "cell_type": "code",
   "id": "c8db9727c42b4a45",
   "metadata": {},
   "source": [
    "# Set up experiment\n",
    "# Before execution this cell\n",
    "# run in terminal:\n",
    "# mlflow server --backend-store-uri sqlite:///data/mlflow.db --port 5005\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5005\")\n",
    "mlflow.set_experiment(\"dspy_evaluation\")\n",
    "\n",
    "# Enable automatic logging for DSPy\n",
    "mlflow.dspy.autolog()\n",
    "print(\"✓ MLflow tracking enabled\")\n",
    "print(\"View results: http://localhost:5005 or http://127.0.0.1:5005\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964ab2b25d93c286",
   "metadata": {},
   "source": [
    "# Configure your values here\n",
    "model_name = 'groq/llama-3.1-8b-instant'\n",
    "api_key = get_api_key('GROQ_API_KEY')\n",
    "api_endpoint = 'https://api.groq.com/openai/v1'\n",
    "useCache = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b936d5e06afd21c6",
   "metadata": {},
   "source": [
    "llm = dspy.c(\n",
    "    model_name,\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    "    cache=useCache\n",
    ")\n",
    "\n",
    "# Set default LLM\n",
    "dspy.settings.configure(lm=llm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2494533eb4face5",
   "metadata": {},
   "source": [
    "ds = load_data('../data/dataset.yaml')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35b799e6a77419e2",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# DEMO: Metric Function (provided)\n",
    "# ========================================\n",
    "# This function evaluates how good a fix is\n",
    "\n",
    "def metric_function(example, prediction, trace=None):\n",
    "    fixed_code = prediction.fixed_code\n",
    "    score, comment = validate_prediction(fixed_code, example['test_case'])\n",
    "    return score\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "123ba44213a90f17",
   "metadata": {},
   "source": [
    "# Baseline: Dummy fixer (returns original code)\n",
    "class DummyFixer(dspy.Module):\n",
    "    \"\"\"A dummy fixer that returns the original code\"\"\"\n",
    "\n",
    "    def forward(self, content, traceback) -> dspy.Prediction:\n",
    "        return dspy.Prediction(\n",
    "            analysis=\"Code analysis\",\n",
    "            fixed_code=content)\n",
    "\n",
    "dummy_fixer = DummyFixer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3e32656a2c00ac7",
   "metadata": {},
   "source": [
    "# Let's map our format to dspy's `Example` type\n",
    "dataset = [dspy.Example(v).with_inputs('content', 'traceback') for v in ds['workshop']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e66a8e2ec4abe5d",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# Your CodeFixer from Section 3\n",
    "# ========================================\n",
    "\n",
    "class AnalyzeSignature(dspy.Signature):\n",
    "    \"\"\"Explain the problem in the code\"\"\"\n",
    "    snippet = dspy.InputField(description=\"Code snippet\")\n",
    "    context = dspy.InputField(description=\"Extra context about issue, like syntax error, etc.\")\n",
    "    summary = dspy.OutputField(description=\"Issue details\")\n",
    "\n",
    "class FixSignature(dspy.Signature):\n",
    "    \"\"\"Fix the code based on analysis\"\"\"\n",
    "    snippet = dspy.InputField(description=\"Code snippet\")\n",
    "    context = dspy.InputField(description=\"Extra context about issue, like syntax error, etc.\")\n",
    "    analysis = dspy.InputField(description=\"Analysis of the issue\")\n",
    "    fixed_code = dspy.OutputField(description=\"Fixed code snippet\")\n",
    "\n",
    "\n",
    "class CodeFixer(dspy.Module):\n",
    "    \"\"\"Module to analyze and fix code issues\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.analyze = dspy.ChainOfThought(AnalyzeSignature)\n",
    "        self.fix = dspy.Predict(FixSignature)\n",
    "\n",
    "    def forward(self, content, traceback) -> dspy.Prediction:\n",
    "        analysis_res = self.analyze(snippet=content, context=traceback)\n",
    "        fix_res = self.fix(snippet=content, context=traceback, analysis=analysis_res.summary)\n",
    "        return dspy.Prediction(\n",
    "            analysis=analysis_res.summary,\n",
    "            fixed_code=fix_res.fixed_code)\n",
    "\n",
    "fixer = CodeFixer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a07f512ae262b5c1",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# DEMO: Evaluation Setup (provided)\n",
    "# ========================================\n",
    "stats = ExperimentStats(dataset)\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=dataset,\n",
    "    metric=metric_function,\n",
    "    display_progress=True,\n",
    "    num_threads=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9793c26e1e5b8c9",
   "metadata": {},
   "source": [
    "# Run evaluation for dummy fixer (baseline) AND track it in MLflow\n",
    "with mlflow.start_run(run_name=\"baseline_dummy\"):\n",
    "    print(\"Evaluating dummy fixer (baseline)...\")\n",
    "    dummy_result = evaluate(dummy_fixer)\n",
    "    stats.add_experiment('dummy', dummy_result)\n",
    "\n",
    "    # MLflow auto-logs metrics, but we can add custom info\n",
    "    mlflow.log_param(\"fixer_type\", \"dummy\")\n",
    "    mlflow.log_metric(\"pass_rate\", dummy_result.score / 100)\n",
    "\n",
    "print(f\"✓ Logged to MLflow: baseline_dummy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stats.get_stats()",
   "id": "b0d4778e4d060a37"
  },
  {
   "cell_type": "code",
   "id": "23c6ea631380cc5b",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# TASK 1: Evaluate Your CodeFixer (with MLflow)\n",
    "# ========================================\n",
    "# Run evaluation AND track it in MLflow\n",
    "#\n",
    "# TODO:\n",
    "# 1. Start an MLflow run with name \"my_codefixer\"\n",
    "# 2. Run evaluate(fixer)\n",
    "# 3. Add to stats\n",
    "# 4. Log the pass rate to MLflow\n",
    "# ========================================\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "with mlflow.start_run(run_name=\"my_codefixer\"):\n",
    "    result = evaluate(fixer)\n",
    "    stats.add_experiment('my_fixer', result)\n",
    "    mlflow.log_metric(\"pass_rate\", result.score / 100)\n",
    "    mlflow.log_param(\"fixer_type\", \"codefixer_v1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stats.get_stats()",
   "id": "9a578b2a8c20184b"
  },
  {
   "cell_type": "code",
   "id": "69f468a792c65c0",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# Analyze Results\n",
    "# ========================================\n",
    "# Look at the stat table and ML Flow UI above\n",
    "#\n",
    "# Questions to discuss:\n",
    "# 1. Which types of errors does your fixer handle best?\n",
    "# 2. Which examples still fail? Why?\n",
    "# 3. What's the score improvement over baseline?\n",
    "# ========================================"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
