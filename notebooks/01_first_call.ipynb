{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from utils.helper import get_api_key\n",
    "import dspy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964ab2b25d93c286",
   "metadata": {},
   "source": [
    "# Configure your values here\n",
    "model_name = 'groq/llama-3.1-8b-instant'\n",
    "api_key = get_api_key('GROQ_API_KEY')\n",
    "api_endpoint = 'https://api.groq.com/openai/v1'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2aac4735de7c9762",
   "metadata": {},
   "source": [
    "Configure the LLM with dspy, it uses liteLLM under the hood."
   ]
  },
  {
   "cell_type": "code",
   "id": "b936d5e06afd21c6",
   "metadata": {},
   "source": [
    "llm = dspy.LM(\n",
    "    model_name,\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    ")\n",
    "\n",
    "large_llm = dspy.LM(\n",
    "    'groq/openai/gpt-oss-20b',\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d54289ad67ce119",
   "metadata": {},
   "source": [
    "llm(\"What is DSPy?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fa5f698f6f89123",
   "metadata": {},
   "source": [
    "Let's set the LLM as the default model for dspy."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb02bc74b08cb7a9",
   "metadata": {},
   "source": [
    "dspy.settings.configure(lm=llm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "91bcbadef1f5f19",
   "metadata": {},
   "source": [
    "Now let's create a simple prediction with dspy."
   ]
  },
  {
   "cell_type": "code",
   "id": "d0cd75697d54b676",
   "metadata": {},
   "source": [
    "qa = dspy.Predict(\"question -> answer\")\n",
    "# parameters and response have declared fields\n",
    "response = qa(question=\"What is DSPy?\")\n",
    "response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c27447ff68f8cf3",
   "metadata": {},
   "source": [
    "# Run the same prediction with a different LLM\n",
    "with dspy.context(lm=large_llm):\n",
    "    response = qa(question=\"What is DSPy?\")\n",
    "response.answer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb7f0485fd97496f",
   "metadata": {},
   "source": [
    "TODO: Make a call asking, \"Explain LLMs in one sentence\"\n",
    "\n",
    "> Hint: Use the qa predictor from above"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e66a8e2ec4abe5d",
   "metadata": {},
   "source": [
    "# Your code here:"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4637e1271d6bdd61",
   "metadata": {},
   "source": [
    "Let's try to ask the same question 3 times.\n",
    "\n",
    "Try to run the code below twice: once with caching enabled and once with caching disabled.\n",
    "Notice the difference in response time."
   ]
  },
  {
   "cell_type": "code",
   "id": "6d2ff08bb423c839",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# TASK 1: Temperature Experimentation\n",
    "# ========================================\n",
    "# Temperature controls randomness:\n",
    "# - 0.0 = deterministic (same answer every time)\n",
    "# - 1.0 = creative (different answers)\n",
    "#\n",
    "# Observe how the answers differ\n",
    "# ========================================\n",
    "\n",
    "temperature=0.01\n",
    "\n",
    "llm_creative = dspy.LM(\n",
    "    model_name,\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    "    temperature=temperature,\n",
    "    cache=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "800db8bce86a978f",
   "metadata": {},
   "source": [
    "**Tips**\n",
    "\n",
    "_Use the `dspy.context` manager to temporarily override the LLM settings._"
   ]
  },
  {
   "cell_type": "code",
   "id": "1497c197b7d75005",
   "metadata": {},
   "source": [
    "# Provide your solution here\n",
    "# TODO: Run the same question 3 times with different `temperature` values, like 0.1, 0.5, 0.9, or play with `cache` parameter."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4daf6b4a8267a875",
   "metadata": {},
   "source": [
    "Check LiteLLM or dspy.LM documentation for more details on configuration options.\n",
    "Let me put attention to a few parameters:\n",
    "- `cache`: The cache to use. By default, it uses the in-memory cache.\n",
    "- `temperature`: The temperature to use for generation. 0 is deterministic, 1 is random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
