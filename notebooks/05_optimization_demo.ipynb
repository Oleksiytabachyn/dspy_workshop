{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from utils.helper import get_api_key, validate_prediction, load_data, ExperimentStats, load_cache, save_cache\n",
    "import dspy\n",
    "import mlflow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3204b5d73347303",
   "metadata": {},
   "source": [
    "# Set up experiment\n",
    "# Before execution this cell\n",
    "# run in terminal:\n",
    "# mlflow server --backend-store-uri sqlite:///data/mlflow.db --port 5005\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5005\")\n",
    "mlflow.set_experiment(\"dspy_optimization\")\n",
    "\n",
    "# Enable automatic logging for DSPy\n",
    "mlflow.dspy.autolog()\n",
    "print(\"✓ MLflow tracking enabled\")\n",
    "print(\"View results: http://localhost:5005 or http://127.0.0.1:5005\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964ab2b25d93c286",
   "metadata": {},
   "source": [
    "# Configure your values here\n",
    "model_name = 'groq/llama-3.1-8b-instant'\n",
    "api_key = get_api_key('GROQ_API_KEY')\n",
    "api_endpoint = 'https://api.groq.com/openai/v1'\n",
    "useCache = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b936d5e06afd21c6",
   "metadata": {},
   "source": [
    "# Original LLM - 8b params model\n",
    "llm = dspy.LM(\n",
    "    model_name,\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    "    cache=useCache\n",
    ")\n",
    "\n",
    "# Configure the large LLM - 20b params model for GEPA optimization\n",
    "large_llm = dspy.LM(\n",
    "    'groq/openai/gpt-oss-20b',\n",
    "    api_key=api_key,\n",
    "    api_base=api_endpoint,\n",
    "    cache=useCache\n",
    ")\n",
    "\n",
    "# Set default LLM\n",
    "dspy.settings.configure(lm=llm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2494533eb4face5",
   "metadata": {},
   "source": [
    "ds = load_data('../data/dataset.yaml')\n",
    "# Load cache if exists\n",
    "load_cache(\"../data/cache.pkl\")\n",
    "\n",
    "# Let's map our format to dspy's `Example` type\n",
    "dataset = [dspy.Example(v).with_inputs('content', 'traceback') for v in ds['workshop']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35b799e6a77419e2",
   "metadata": {},
   "source": [
    "def metric_function(example, prediction, trace=None):\n",
    "    fixed_code = prediction.fixed_code\n",
    "    score, comment = validate_prediction(fixed_code, example['test_case'])\n",
    "    return score\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "123ba44213a90f17",
   "metadata": {},
   "source": [
    "# Baseline: Dummy fixer (returns original code)\n",
    "class DummyFixer(dspy.Module):\n",
    "    \"\"\"A dummy fixer that returns the original code\"\"\"\n",
    "\n",
    "    def forward(self, content, traceback) -> dspy.Prediction:\n",
    "        return dspy.Prediction(\n",
    "            analysis=\"Code analysis\",\n",
    "            fixed_code=content)\n",
    "\n",
    "dummy_fixer = DummyFixer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e66a8e2ec4abe5d",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# Your CodeFixer from Section 3\n",
    "# ========================================\n",
    "\n",
    "class AnalyzeSignature(dspy.Signature):\n",
    "    \"\"\"Explain the problem in the code\"\"\"\n",
    "    snippet = dspy.InputField(description=\"Code snippet\")\n",
    "    context = dspy.InputField(description=\"Extra context about issue, like syntax error, etc.\")\n",
    "    summary = dspy.OutputField(description=\"Issue details\")\n",
    "\n",
    "class FixSignature(dspy.Signature):\n",
    "    \"\"\"Fix the code based on analysis\"\"\"\n",
    "    snippet = dspy.InputField(description=\"Code snippet\")\n",
    "    context = dspy.InputField(description=\"Extra context about issue, like syntax error, etc.\")\n",
    "    analysis = dspy.InputField(description=\"Analysis of the issue\")\n",
    "    fixed_code = dspy.OutputField(description=\"Fixed code snippet\")\n",
    "\n",
    "\n",
    "class CodeFixer(dspy.Module):\n",
    "    \"\"\"Module to analyze and fix code issues\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.analyze = dspy.ChainOfThought(AnalyzeSignature)\n",
    "        self.fix = dspy.Predict(FixSignature)\n",
    "\n",
    "    def forward(self, content, traceback) -> dspy.Prediction:\n",
    "        analysis_res = self.analyze(snippet=content, context=traceback)\n",
    "        fix_res = self.fix(snippet=content, context=traceback, analysis=analysis_res.summary)\n",
    "        return dspy.Prediction(\n",
    "            analysis=analysis_res.summary,\n",
    "            fixed_code=fix_res.fixed_code)\n",
    "\n",
    "stats = ExperimentStats(dataset)\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=dataset,\n",
    "    metric=metric_function,\n",
    "    display_progress=True,\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "fixer = CodeFixer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9793c26e1e5b8c9",
   "metadata": {},
   "source": [
    "with mlflow.start_run(run_name=\"baseline_dummy\"):\n",
    "    print(\"Evaluating dummy fixer (baseline)...\")\n",
    "    dummy_result = evaluate(dummy_fixer)\n",
    "    stats.add_experiment('dummy', dummy_result)\n",
    "    mlflow.log_metric(\"pass_rate\", dummy_result.score / 100)\n",
    "    mlflow.log_param(\"fixer_type\", \"dummy\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"my_codefixer\"):\n",
    "    result = evaluate(fixer)\n",
    "    stats.add_experiment('fixer', result)\n",
    "    mlflow.log_metric(\"pass_rate\", result.score / 100)\n",
    "    mlflow.log_param(\"fixer_type\", \"codefixer_v1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20dda3f1eeb4236f",
   "metadata": {},
   "source": [
    "stats.get_stats()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e8df5e4ef7c6d8",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# INSTRUCTOR DEMO: MIPROv2 Optimization\n",
    "# ========================================\n",
    "# This will take ~2-3 minutes to run\n",
    "# Watch the trials and observe the scores\n",
    "# ========================================\n",
    "\n",
    "optimizer_mipro = dspy.MIPROv2(\n",
    "    metric=metric_function,\n",
    "    auto=\"light\"  # Light mode for speed\n",
    ")\n",
    "\n",
    "print(\"Starting optimization...\")\n",
    "print(\"This will try different instructions and few-shot examples\")\n",
    "print(\"Watch the trial scores improve!\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"optimized_by_MIPRO\"):\n",
    "    optimized_MIPRO_fixer = optimizer_mipro.compile(\n",
    "        fixer,\n",
    "        trainset=dataset,\n",
    "        valset=dataset,\n",
    "        requires_permission_to_run=False\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Optimization complete!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "402371bcb35ed277",
   "metadata": {},
   "source": [
    "with mlflow.start_run(run_name=\"evaluate_optimized_by_MIPRO\"):\n",
    "    optimized_mipro_result = evaluate(optimized_MIPRO_fixer)\n",
    "    stats.add_experiment('optimized_mipro', optimized_mipro_result)\n",
    "    mlflow.log_metric(\"pass_rate\", optimized_mipro_result.score / 100)\n",
    "    mlflow.log_param(\"fixer_type\", \"codefixer_v2_optimized_by_MIPRO\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84a2b97c-69b5-4d59-b5bd-812dc70c11af",
   "metadata": {},
   "source": [
    "stats.get_stats()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54db172c2149103f",
   "metadata": {},
   "source": [
    "# GEPA Optimization\n",
    "# Metric function that provides detailed feedback for large LLM reflection\n",
    "def gepa_metric_function(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    analysis=prediction.analysis,\n",
    "    fixed_code=prediction.fixed_code\n",
    "    score, comment = validate_prediction(fixed_code, example['test_case'])\n",
    "    return dspy.Prediction(score=score, feedback=f\"{comment}\\n{analysis}\")\n",
    "\n",
    "optimizer_gepa = dspy.GEPA(\n",
    "    metric=gepa_metric_function,\n",
    "    auto=\"light\",\n",
    "    reflection_lm=large_llm\n",
    ")\n",
    "\n",
    "\n",
    "# Might take ~5-7 minutes to run\n",
    "print(\"Starting GEPA optimization...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"optimized_by_GEPA\"):\n",
    "    optimized_gepa_fixer = optimizer_gepa.compile(\n",
    "        fixer,\n",
    "        trainset=dataset,\n",
    "        valset=dataset,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da23c6b43cf74f66",
   "metadata": {},
   "source": [
    "with mlflow.start_run(run_name=\"evaluate_optimized_by_MIPRO\"):\n",
    "    optimized_gepa_result = evaluate(optimized_gepa_fixer)\n",
    "    stats.add_experiment('optimized_gepa', optimized_gepa_result)\n",
    "    mlflow.log_metric(\"pass_rate\", optimized_gepa_result.score / 100)\n",
    "    mlflow.log_param(\"fixer_type\", \"codefixer_v3_optimized_by_GEPA\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36f3c66a262f852",
   "metadata": {},
   "source": [
    "stats.get_stats()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3beedea87da27895",
   "metadata": {},
   "source": [
    "# save_cache('../data/cache.pkl')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3099a327a85b941",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a79792c41436f58c",
   "metadata": {},
   "source": [
    "optimized_MIPRO_fixer"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
